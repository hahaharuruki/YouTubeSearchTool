import subprocess
import whisper
import faiss
import numpy as np
import json
import os
import sys
from tqdm import tqdm

# ========== 1. å‹•ç”»ã‹ã‚‰éŸ³å£°ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ==========
def download_audio(youtube_url, output_path="output.mp3"):
    if os.path.exists(output_path):
        print(f"ğŸ—‘ï¸ æ—¢å­˜ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ« {output_path} ã‚’å‰Šé™¤ã—ã¾ã™...")
        os.remove(output_path)

    print("ğŸ§ éŸ³å£°ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
    subprocess.run([
        "yt-dlp",
        "--no-cache-dir",
        "-x", "--audio-format", "mp3",
        "-o", output_path,
        youtube_url
    ])
    print(f"âœ… éŸ³å£°ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")

# ========== 2. Whisperã§æ–‡å­—èµ·ã“ã— ==========
def transcribe_audio(audio_path):
    print("ğŸ“ Whisperã§æ–‡å­—èµ·ã“ã—ä¸­...")
    model = whisper.load_model("large-v3")  # "small" ã‚„ "medium" ã«å¤‰ãˆã¦ã‚‚OK
    result = model.transcribe(audio_path)
    segments = result["segments"]
    print(f"âœ… {len(segments)}ä»¶ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å–å¾—")

    # 10ç§’ä»¥ä¸Šã«ãªã‚‹ã¾ã§ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’é€£çµ
    chunks = []
    current_chunk = {
        "start": segments[0]["start"],
        "end": segments[0]["end"],
        "text": segments[0]["text"]
    }
    current_speaker = "Speaker 1"
    for seg in segments[1:]:
        current_chunk["end"] = seg["end"]
        current_chunk["text"] += " " + seg["text"]
        duration = current_chunk["end"] - current_chunk["start"]
        if duration >= 10:
            current_chunk["speaker"] = current_speaker
            chunks.append(current_chunk)
            current_speaker = "Speaker 2" if current_speaker == "Speaker 1" else "Speaker 1"
            current_chunk = {
                "start": seg["start"],
                "end": seg["end"],
                "text": seg["text"]
            }
    current_chunk["speaker"] = current_speaker
    chunks.append(current_chunk)

    print(f"âœ… {len(chunks)}ä»¶ã®ãƒãƒ£ãƒ³ã‚¯ã«å†æ§‹æˆã—ã¾ã—ãŸ")
    return chunks

# ========== 3. åŸ‹ã‚è¾¼ã¿ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰ ==========
def embed_text(text):
    import torch
    from transformers import AutoTokenizer, AutoModel
    
    # ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›ï¼ˆé‡ã¿ä»˜ã‘ï¼‰
    models = [
        ("BAAI/bge-large-en-v1.5", 0.6),  # åŸºæœ¬ãƒ¢ãƒ‡ãƒ«
        ("intfloat/multilingual-e5-large", 0.4)  # æ—¥æœ¬èªå¯¾å¿œè£œå®Œ
    ]
    
    final_embedding = None
    device = torch.device("cpu")  # MPSã¯ä¸å®‰å®šãªã®ã§CPUæ¨å¥¨
    
    for model_name, weight in models:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name).to(device)
        
        inputs = tokenizer(
            text, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512, 
            padding=True
        ).to(device)
        
        with torch.no_grad():
            outputs = model(**inputs)
            embedding = outputs.last_hidden_state[:, 0].cpu().numpy()
        
        if final_embedding is None:
            final_embedding = embedding * weight
        else:
            # æ¬¡å…ƒãŒç•°ãªã‚‹å ´åˆã®èª¿æ•´ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
            if embedding.shape != final_embedding.shape:
                from sklearn.preprocessing import normalize
                # ãƒªã‚µã‚¤ã‚ºã—ã¦æ­£è¦åŒ–
                embedding = normalize(embedding).reshape(final_embedding.shape)
            
            final_embedding += embedding * weight
        
        # ãƒ¡ãƒ¢ãƒªè§£æ”¾
        del model, tokenizer
        import gc
        gc.collect()
    
    return final_embedding

# ========== 4. FAISS ã«æ ¼ç´ ==========
def save_to_faiss(embeddings, segments, index_path="faiss.index", meta_path="metadata.json"):
    # Load existing index if exists
    if os.path.exists(index_path):
        print(f"ğŸ“‚ æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {index_path} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        index = faiss.read_index(index_path)
    else:
        dim = embeddings.shape[1]
        index = faiss.IndexFlatL2(dim)

    # Load existing metadata if exists
    if os.path.exists(meta_path):
        print(f"ğŸ“‚ æ—¢å­˜ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ {meta_path} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        with open(meta_path, "r", encoding="utf-8") as f:
            existing_metadata = json.load(f)
    else:
        existing_metadata = []

    # Prepare a set of existing entries for quick lookup to avoid duplicates
    existing_set = set((m["start"], m["end"], m["text"]) for m in existing_metadata)

    # Filter new segments and embeddings to exclude duplicates
    new_metadata = []
    new_embeddings = []
    for seg, emb in zip(segments, embeddings):
        key = (seg["start"], seg["end"], seg["text"])
        if key not in existing_set:
            new_metadata.append(seg)
            new_embeddings.append(emb)
            existing_set.add(key)

    if len(new_embeddings) > 0:
        new_embeddings_np = np.array(new_embeddings).astype("float32")
        index.add(new_embeddings_np)
        existing_metadata.extend(new_metadata)
        print(f"â• {len(new_embeddings)}ä»¶ã®æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
    else:
        print("â„¹ï¸ æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆé‡è¤‡ãªã—ï¼‰")

    print(f"ğŸ’¾ ãƒ™ã‚¯ãƒˆãƒ«ã‚’ {index_path} ã«ä¿å­˜ä¸­...")
    faiss.write_index(index, index_path)

    print(f"ğŸ’¾ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ {meta_path} ã«ä¿å­˜ä¸­...")
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(existing_metadata, f, ensure_ascii=False, indent=2)

    print("âœ… ä¿å­˜å®Œäº†")

# ========== å®Ÿè¡Œãƒ•ãƒ­ãƒ¼ ==========
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ä½¿ã„æ–¹: python full_pipeline.py <YouTubeã®URL>")
        sys.exit(1)

    url = sys.argv[1]
    download_audio(url)
    segments = transcribe_audio("output.mp3")
    embeddings = [embed_text(seg["text"]).squeeze() for seg in segments]
    embeddings = np.stack(embeddings).astype("float32")
    save_to_faiss(embeddings, segments)

# Silero VADã¸å¤‰æ›´ï¼ˆç²¾åº¦å‘ä¸Šï¼‰
def get_vad_segments(audio_path):
    import torch
    model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',
                                 model='silero_vad',
                                 force_reload=True)
    (get_speech_timestamps, _, _, _, _) = utils
    
    # ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã®èª­ã¿è¾¼ã¿ï¼ˆ16kHzã€ãƒ¢ãƒãƒ©ãƒ«åŒ–ï¼‰
    import librosa
    audio, sr = librosa.load(audio_path, sr=16000, mono=True)
    
    # éŸ³å£°åŒºé–“ã®æ¤œå‡º
    speech_timestamps = get_speech_timestamps(
        torch.tensor(audio),
        model,
        threshold=0.5,  # æ„Ÿåº¦èª¿æ•´ï¼ˆ0.3-0.7ã®é–“ã§èª¿æ•´å¯èƒ½ï¼‰
        sampling_rate=16000
    )
    
    return speech_timestamps

# HNSWã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¸ã®å¤‰æ›´ï¼ˆç²¾åº¦ã¨é€Ÿåº¦ã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
def create_improved_index(embeddings):
    import faiss
    import numpy as np
    
    dim = embeddings.shape[1]
    
    # HNSWã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ§‹ç¯‰
    # M: ã‚°ãƒ©ãƒ•ã®æ¬¡æ•°ï¼ˆé«˜ã„ã»ã©ç²¾åº¦å‘ä¸Šã ãŒãƒ¡ãƒ¢ãƒªæ¶ˆè²»å¢—åŠ ï¼‰
    # efConstruction: æ§‹ç¯‰æ™‚ã®æ¢ç´¢å¹…ï¼ˆé«˜ã„ã»ã©ç²¾åº¦å‘ä¸Šã ãŒæ§‹ç¯‰æ™‚é–“å¢—åŠ ï¼‰
    index = faiss.IndexHNSWFlat(dim, M=32)
    index.hnsw.efConstruction = 80  # æ§‹ç¯‰æ™‚ã®æ¢ç´¢å¹…
    index.hnsw.efSearch = 128  # æ¤œç´¢æ™‚ã®æ¢ç´¢å¹…ï¼ˆé«˜ã„ã»ã©ç²¾åº¦å‘ä¸Šï¼‰
    
    # åŸ‹ã‚è¾¼ã¿ã‚’è¿½åŠ 
    if len(embeddings) > 0:
        faiss.normalize_L2(embeddings)  # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®ãŸã‚ã®æ­£è¦åŒ–
        index.add(embeddings)
    
    return index

# ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã§ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆç²¾åº¦å‘ä¸Šï¼‰
def rerank_results(query, results, metadata, top_k=5):
    from transformers import AutoModelForSequenceClassification, AutoTokenizer
    import torch
    
    # å¤šè¨€èªã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
    model_name = "amberoad/bert-multilingual-passage-reranking-msmarco"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    
    pairs = []
    ids = []
    
    for idx in results:
        if idx >= 0 and idx < len(metadata):
            text = metadata[idx]["text"]
            pairs.append([query, text])
            ids.append(idx)
    
    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã—ã¦å‡¦ç†
    batch_size = 4
    scores = []
    
    for i in range(0, len(pairs), batch_size):
        batch = pairs[i:i+batch_size]
        inputs = tokenizer(
            batch, 
            padding=True, 
            truncation=True, 
            return_tensors="pt", 
            max_length=512
        )
        
        with torch.no_grad():
            outputs = model(**inputs)
            batch_scores = outputs.logits.squeeze(-1).tolist()
            scores.extend(batch_scores)
    
    # ã‚¹ã‚³ã‚¢ã¨å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ã‚½ãƒ¼ãƒˆ
    scored_results = sorted(zip(ids, scores), key=lambda x: x[1], reverse=True)
    return [idx for idx, _ in scored_results[:top_k]]