import subprocess
import whisper
import faiss
import numpy as np
import json
import os
import sys
from tqdm import tqdm

# ========== 1. å‹•ç”»ã‹ã‚‰éŸ³å£°ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ==========
def download_audio(youtube_url, output_path="output.mp3"):
    if os.path.exists(output_path):
        print(f"ğŸ—‘ï¸ æ—¢å­˜ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ« {output_path} ã‚’å‰Šé™¤ã—ã¾ã™...")
        os.remove(output_path)

    print("ğŸ§ éŸ³å£°ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
    subprocess.run([
        "yt-dlp",
        "--no-cache-dir",
        "-x", "--audio-format", "mp3",
        "-o", output_path,
        youtube_url
    ])
    print(f"âœ… éŸ³å£°ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")

# ========== 2. Whisperã§æ–‡å­—èµ·ã“ã— ==========
def transcribe_audio(audio_path):
    print("ğŸ“ Whisperã§æ–‡å­—èµ·ã“ã—ä¸­...")
    model = whisper.load_model("large-v3")  # "small" ã‚„ "medium" ã«å¤‰ãˆã¦ã‚‚OK
    result = model.transcribe(audio_path)
    segments = result["segments"]
    print(f"âœ… {len(segments)}ä»¶ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å–å¾—")

    # 10ç§’ä»¥ä¸Šã«ãªã‚‹ã¾ã§ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’é€£çµ
    chunks = []
    current_chunk = {
        "start": segments[0]["start"],
        "end": segments[0]["end"],
        "text": segments[0]["text"]
    }
    current_speaker = "Speaker 1"
    for seg in segments[1:]:
        current_chunk["end"] = seg["end"]
        current_chunk["text"] += " " + seg["text"]
        duration = current_chunk["end"] - current_chunk["start"]
        if duration >= 10:
            current_chunk["speaker"] = current_speaker
            chunks.append(current_chunk)
            current_speaker = "Speaker 2" if current_speaker == "Speaker 1" else "Speaker 1"
            current_chunk = {
                "start": seg["start"],
                "end": seg["end"],
                "text": seg["text"]
            }
    current_chunk["speaker"] = current_speaker
    chunks.append(current_chunk)

    print(f"âœ… {len(chunks)}ä»¶ã®ãƒãƒ£ãƒ³ã‚¯ã«å†æ§‹æˆã—ã¾ã—ãŸ")
    return chunks

# ========== 3. åŸ‹ã‚è¾¼ã¿ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰ ==========
def embed_texts(segments):
    from transformers import AutoTokenizer, AutoModel
    import torch

    tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-large-en-v1.5")
    model = AutoModel.from_pretrained("BAAI/bge-large-en-v1.5")

    texts = [seg["text"] for seg in segments]
    print("ğŸ”¢ ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ä¸­...")

    embeddings = []
    for text in texts:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
        with torch.no_grad():
            model_output = model(**inputs)
        sentence_embedding = model_output.last_hidden_state[:, 0]
        embeddings.append(sentence_embedding.squeeze().numpy())

    return embeddings

# ========== 4. FAISS ã«æ ¼ç´ ==========
def save_to_faiss(embeddings, segments, index_path="faiss.index", meta_path="metadata.json"):
    # Load existing index if exists
    if os.path.exists(index_path):
        print(f"ğŸ“‚ æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {index_path} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        index = faiss.read_index(index_path)
    else:
        dim = embeddings.shape[1]
        index = faiss.IndexFlatL2(dim)

    # Load existing metadata if exists
    if os.path.exists(meta_path):
        print(f"ğŸ“‚ æ—¢å­˜ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ {meta_path} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        with open(meta_path, "r", encoding="utf-8") as f:
            existing_metadata = json.load(f)
    else:
        existing_metadata = []

    # Prepare a set of existing entries for quick lookup to avoid duplicates
    existing_set = set((m["start"], m["end"], m["text"]) for m in existing_metadata)

    # Filter new segments and embeddings to exclude duplicates
    new_metadata = []
    new_embeddings = []
    for seg, emb in zip(segments, embeddings):
        key = (seg["start"], seg["end"], seg["text"])
        if key not in existing_set:
            new_metadata.append(seg)
            new_embeddings.append(emb)
            existing_set.add(key)

    if len(new_embeddings) > 0:
        new_embeddings_np = np.array(new_embeddings).astype("float32")
        index.add(new_embeddings_np)
        existing_metadata.extend(new_metadata)
        print(f"â• {len(new_embeddings)}ä»¶ã®æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
    else:
        print("â„¹ï¸ æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆé‡è¤‡ãªã—ï¼‰")

    print(f"ğŸ’¾ ãƒ™ã‚¯ãƒˆãƒ«ã‚’ {index_path} ã«ä¿å­˜ä¸­...")
    faiss.write_index(index, index_path)

    print(f"ğŸ’¾ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ {meta_path} ã«ä¿å­˜ä¸­...")
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(existing_metadata, f, ensure_ascii=False, indent=2)

    print("âœ… ä¿å­˜å®Œäº†")

# ========== å®Ÿè¡Œãƒ•ãƒ­ãƒ¼ ==========
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ä½¿ã„æ–¹: python full_pipeline.py <YouTubeã®URL>")
        sys.exit(1)

    url = sys.argv[1]
    download_audio(url)
    segments = transcribe_audio("output.mp3")
    embeddings = embed_texts(segments)
    save_to_faiss(np.array(embeddings).astype("float32"), segments)