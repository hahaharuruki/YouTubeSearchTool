import torch
from typing import List
# Ensure compatibility: define get_default_device if missing
if not hasattr(torch, "get_default_device"):
    torch.get_default_device = lambda: torch.device("cpu")

import subprocess
import faiss
import whisperx # WhisperXã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import numpy as np
import json
import os
import sys
from tqdm import tqdm
import noisereduce as nr
import soundfile as sf

# Global cache for models to avoid reloading
_vad_model_cache = None
_vad_utils_cache = None
# WhisperXãƒ¢ãƒ‡ãƒ«ç”¨ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥
_whisperx_asr_model_cache = None
_whisperx_align_model_cache = None
_whisperx_align_metadata_cache = None
_whisperx_diarize_pipeline_cache = None

# Silero VADã¸å¤‰æ›´ï¼ˆç²¾åº¦å‘ä¸Šï¼‰
def get_vad_segments(audio_path_or_array, sr_target=16000):
    global _vad_model_cache, _vad_utils_cache
    import torch
    import librosa

    if _vad_model_cache is None or _vad_utils_cache is None:
        print("ğŸ”Š Silero VADãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        _vad_model_cache, _vad_utils_cache = torch.hub.load(repo_or_dir='snakers4/silero-vad',
                                                             model='silero_vad',
                                                             force_reload=False, # Use cached model after first download
                                                             trust_repo=True) # Recommended for hub.load

    (get_speech_timestamps, _, _, _, _) = _vad_utils_cache

    if isinstance(audio_path_or_array, str):
        # ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã®èª­ã¿è¾¼ã¿ï¼ˆæŒ‡å®šã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆã€ãƒ¢ãƒãƒ©ãƒ«åŒ–ï¼‰
        audio_np, sr_orig = librosa.load(audio_path_or_array, sr=None, mono=True) # Load original sr
        if sr_orig != sr_target:
            print(f"ğŸ¤ éŸ³å£°ã‚’ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ä¸­: {sr_orig}Hz -> {sr_target}Hz")
            audio_np = librosa.resample(audio_np, orig_sr=sr_orig, target_sr=sr_target)
        current_sr = sr_target
    elif isinstance(audio_path_or_array, np.ndarray):
        audio_np = audio_path_or_array
        # Assuming if a numpy array is passed, its sample rate is sr_target
        # This should be ensured by the caller or sr should be passed.
        # For this pipeline, get_vad_segments is called with a path first.
        current_sr = sr_target # Assume sr_target if ndarray
    else:
        raise ValueError("audio_path_or_arrayã¯ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹(str)ã¾ãŸã¯Numpyé…åˆ—ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")

    audio_tensor = torch.from_numpy(audio_np)

    # éŸ³å£°åŒºé–“ã®æ¤œå‡º
    speech_timestamps = get_speech_timestamps(
        audio_tensor,
        _vad_model_cache,
        threshold=0.5,  # æ„Ÿåº¦èª¿æ•´ï¼ˆ0.3-0.7ã®é–“ã§èª¿æ•´å¯èƒ½ï¼‰
        sampling_rate=current_sr # VADãƒ¢ãƒ‡ãƒ«ãŒæœŸå¾…ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ
    )
    # speech_timestamps ã¯ current_sr ã§ã®ã‚µãƒ³ãƒ—ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆ
    return speech_timestamps, audio_np, current_sr

# ========== ãƒã‚¤ã‚ºé™¤å» ==========
def reduce_noise(audio_path: str, output_path: str):
    """
    éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒã‚¤ã‚ºã‚’é™¤å»ã—ã€æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã«ä¿å­˜ã™ã‚‹ã€‚
    """
    import librosa
    print(f"ğŸ”Š ãƒã‚¤ã‚ºé™¤å»ä¸­: {audio_path} ...")
    
    # librosaã§éŸ³å£°ã‚’èª­ã¿è¾¼ã¿ (16kHzãƒ¢ãƒãƒ©ãƒ«ã«çµ±ä¸€)
    audio, sr = librosa.load(audio_path, sr=16000, mono=True)
    
    # ãƒã‚¤ã‚ºé™¤å»ã‚’å®Ÿè¡Œ (å®šå¸¸çš„ãªãƒã‚¤ã‚ºã‚’æƒ³å®š)
    reduced_noise_audio = nr.reduce_noise(y=audio, sr=sr, stationary=True)
    
    # å‡¦ç†å¾Œã®éŸ³å£°ã‚’ä¿å­˜
    sf.write(output_path, reduced_noise_audio, sr)
    print(f"âœ… ãƒã‚¤ã‚ºé™¤å»æ¸ˆã¿éŸ³å£°ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")

# ========== 2. Whisperã§æ–‡å­—èµ·ã“ã— ==========
def transcribe_audio(audio_path: str, language_code: str = "ja") -> List[dict]:
    """
    WhisperXã‚’ä½¿ç”¨ã—ã¦éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ–‡å­—èµ·ã“ã—ã¨è©±è€…åˆ†é›¢ã‚’è¡Œã†ã€‚
    """
    global _whisperx_asr_model_cache, _whisperx_align_model_cache, \
           _whisperx_align_metadata_cache, _whisperx_diarize_pipeline_cache
    
    import torch # for device selection

    device = "cuda" if torch.cuda.is_available() else "cpu"
    # CPUã®å ´åˆã€float32ã®æ–¹ãŒå®‰å®šã¾ãŸã¯é«˜é€Ÿãªå ´åˆãŒã‚ã‚Šã¾ã™ãŒã€int8ã¯ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒè‰¯ã„ã§ã™
    compute_type = "float16" if device == "cuda" else "int8" 
    batch_size = 16 # GPUãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦èª¿æ•´

    print(f"ğŸš€ WhisperX (large-v3) ã‚’ä½¿ç”¨ã—ã¦å‡¦ç†é–‹å§‹ (device: {device}, compute_type: {compute_type})")

    # 1. ASRãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
    if _whisperx_asr_model_cache is None:
        print(f"ğŸ¤« WhisperX ASRãƒ¢ãƒ‡ãƒ« (large-v3, lang={language_code or 'auto'}) ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        # language_codeãŒNoneã®å ´åˆã€WhisperXã¯è‡ªå‹•æ¤œå‡ºã‚’è©¦ã¿ã¾ã™
        _whisperx_asr_model_cache = whisperx.load_model("large-v3", device, compute_type=compute_type, language=language_code)

    # 2. éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
    print(f"ğŸ§ WhisperXç”¨ã«éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {audio_path}")
    audio_input = whisperx.load_audio(audio_path) # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§16kHzã«ãƒªã‚µãƒ³ãƒ—ãƒ«

    # 3. æ–‡å­—èµ·ã“ã—å®Ÿè¡Œ
    print("ğŸ“ WhisperXã§æ–‡å­—èµ·ã“ã—ä¸­...")
    result = _whisperx_asr_model_cache.transcribe(audio_input, batch_size=batch_size)
    transcription_language = result["language"]
    print(f"ğŸŒ WhisperXãŒä½¿ç”¨/æ¤œå‡ºã—ãŸè¨€èª: {transcription_language}")

    # 4. ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨å®Ÿè¡Œ (æ­£ç¢ºãªå˜èªã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®ãŸã‚)
    # ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«ã¯è¨€èªã«ä¾å­˜ã™ã‚‹ãŸã‚ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚‚è¨€èªã‚’è€ƒæ…®
    if _whisperx_align_model_cache is None or \
       _whisperx_align_metadata_cache is None or \
       _whisperx_align_metadata_cache.get("language_code") != transcription_language:
        print(f"ğŸ”„ WhisperX Alignãƒ¢ãƒ‡ãƒ« ({transcription_language}) ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        try:
            _whisperx_align_model_cache, _whisperx_align_metadata_cache = whisperx.load_align_model(
                language_code=transcription_language, device=device
            )
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±ã«è¨€èªã‚³ãƒ¼ãƒ‰ã‚’ä¿å­˜
            if _whisperx_align_metadata_cache: # metadataãŒNoneã§ãªã„ã“ã¨ã‚’ç¢ºèª
                 _whisperx_align_metadata_cache["language_code"] = transcription_language
        except Exception as e:
            print(f"âŒ Alignãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ ({transcription_language}): {e}")
            print("è©±è€…åˆ†é›¢ãªã—ã§å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™ã€‚")
            # ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            aligned_result = {"segments": result["segments"]} # å…ƒã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ä½¿ç”¨
    
    if _whisperx_align_model_cache: # ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«ãŒæ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸå ´åˆ
        print("ğŸ”„ WhisperXã§ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆå‡¦ç†ä¸­...")
        aligned_result = whisperx.align(result["segments"], _whisperx_align_model_cache, _whisperx_align_metadata_cache, audio_input, device, return_char_alignments=False)
    else: # ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸå ´åˆ
        print("âš ï¸ ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        aligned_result = {"segments": result["segments"]}

    # 5. è©±è€…åˆ†é›¢ã®å®Ÿè¡Œ
    final_segments_for_chunking = aligned_result["segments"]
    hf_token = os.getenv("HF_TOKEN")
    if not hf_token:
        print("âš ï¸ Hugging Faceãƒˆãƒ¼ã‚¯ãƒ³ (HF_TOKEN) ãŒç’°å¢ƒå¤‰æ•°ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è©±è€…åˆ†é›¢ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
    else:
        if _whisperx_diarize_pipeline_cache is None:
            print("ğŸ—£ï¸ WhisperX Diarization ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­ (pyannote.audio)...")
            from whisperx.diarize import DiarizationPipeline # å¤‰æ›´ç‚¹
            _whisperx_diarize_pipeline_cache = DiarizationPipeline(use_auth_token=hf_token, device=device)
        
        print("ğŸ—£ï¸ WhisperXã§è©±è€…åˆ†é›¢ã‚’å®Ÿè¡Œä¸­...")
        # DiarizationPipelineã¯éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¾ãŸã¯ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚’å—ã‘å–ã‚Œã¾ã™
        # audio_path (ãƒã‚¤ã‚ºé™¤å»å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«) ã‚’ä½¿ç”¨ã™ã‚‹ã®ãŒä¸€èˆ¬çš„
        diarize_segments = _whisperx_diarize_pipeline_cache(audio_path) 
        
        print("ğŸ¤ è©±è€…æƒ…å ±ã‚’æ–‡å­—èµ·ã“ã—çµæœã«å‰²ã‚Šå½“ã¦ä¸­...")
        result_with_speakers = whisperx.assign_word_speakers(diarize_segments, aligned_result)
        final_segments_for_chunking = result_with_speakers["segments"]

    # 6. ãƒãƒ£ãƒ³ã‚¯ã®æ•´å½¢
    chunks = []
    print("ğŸ’¬ æ–‡å­—èµ·ã“ã—çµæœã‚’ãƒãƒ£ãƒ³ã‚¯ã«æ•´å½¢ä¸­...")
    for seg in tqdm(final_segments_for_chunking, desc="ãƒãƒ£ãƒ³ã‚¯æ•´å½¢"):
        chunks.append({
            "start": seg["start"],
            "end": seg["end"],
            "text": seg["text"].strip(),
            "speaker": seg.get("speaker", "UNKNOWN") # è©±è€…æƒ…å ±ãŒãªã„å ´åˆã¯UNKNOWN
        })
    
    print(f"âœ… {len(chunks)}ä»¶ã®ãƒãƒ£ãƒ³ã‚¯ã«å†æ§‹æˆã—ã¾ã—ãŸï¼ˆè©±è€…æƒ…å ± {'ã‚ã‚Š' if hf_token and _whisperx_diarize_pipeline_cache else 'ãªã—'}ï¼‰")
    return chunks

# ========== 1. å‹•ç”»ã‹ã‚‰éŸ³å£°ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ==========
def download_audio(youtube_url, output_path="output.mp3"):
    if os.path.exists(output_path):
        print(f"ğŸ—‘ï¸ æ—¢å­˜ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ« {output_path} ã‚’å‰Šé™¤ã—ã¾ã™...")
        os.remove(output_path)

    print("ğŸ§ éŸ³å£°ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
    subprocess.run([
        "yt-dlp",
        "--no-cache-dir",
        "-x", "--audio-format", "mp3",
        "-o", output_path,
        youtube_url
    ])
    print(f"âœ… éŸ³å£°ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")

# ========== 3. åŸ‹ã‚è¾¼ã¿ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰ ==========
def embed_text(text):
    import torch
    # Ensure compatibility: define get_default_device if missing
    if not hasattr(torch, "get_default_device"):
        torch.get_default_device = lambda: torch.device("cpu")
    from transformers import AutoTokenizer, AutoModel
    
    # ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›ï¼ˆé‡ã¿ä»˜ã‘ï¼‰
    models = [
        ("BAAI/bge-large-en-v1.5", 0.6),  # åŸºæœ¬ãƒ¢ãƒ‡ãƒ«
        ("intfloat/multilingual-e5-large", 0.4)  # æ—¥æœ¬èªå¯¾å¿œè£œå®Œ
    ]
    
    final_embedding = None
    device = torch.device("cpu")  # MPSã¯ä¸å®‰å®šãªã®ã§CPUæ¨å¥¨
    
    for model_name, weight in models:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name).to(device)
        
        inputs = tokenizer(
            text, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512, 
            padding=True
        ).to(device)
        
        with torch.no_grad():
            outputs = model(**inputs)
            embedding = outputs.last_hidden_state[:, 0].cpu().numpy()
        
        if final_embedding is None:
            final_embedding = embedding * weight
        else:
            # æ¬¡å…ƒãŒç•°ãªã‚‹å ´åˆã®èª¿æ•´ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
            if embedding.shape != final_embedding.shape:
                from sklearn.preprocessing import normalize
                # ãƒªã‚µã‚¤ã‚ºã—ã¦æ­£è¦åŒ–
                embedding = normalize(embedding).reshape(final_embedding.shape)
            
            final_embedding += embedding * weight
        
        # ãƒ¡ãƒ¢ãƒªè§£æ”¾
        del model, tokenizer
        import gc
        gc.collect()
    
    return final_embedding

# ========== 4. FAISS ã«æ ¼ç´ ==========
def save_to_faiss(embeddings, segments, index_path="faiss.index", meta_path="metadata.json"):
    # Load existing index if exists
    if os.path.exists(index_path):
        print(f"ğŸ“‚ æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {index_path} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        index = faiss.read_index(index_path)
    else:
        dim = embeddings.shape[1]
        index = faiss.IndexFlatL2(dim)

    # Load existing metadata if exists
    if os.path.exists(meta_path):
        print(f"ğŸ“‚ æ—¢å­˜ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ {meta_path} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        with open(meta_path, "r", encoding="utf-8") as f:
            existing_metadata = json.load(f)
    else:
        existing_metadata = []

    # Prepare a set of existing entries for quick lookup to avoid duplicates
    existing_set = set((m["start"], m["end"], m["text"]) for m in existing_metadata)

    # Filter new segments and embeddings to exclude duplicates
    new_metadata = []
    new_embeddings = []
    for seg, emb in zip(segments, embeddings):
        key = (seg["start"], seg["end"], seg["text"])
        if key not in existing_set:
            new_metadata.append(seg)
            new_embeddings.append(emb)
            existing_set.add(key)

    if len(new_embeddings) > 0:
        new_embeddings_np = np.array(new_embeddings).astype("float32")
        index.add(new_embeddings_np)
        existing_metadata.extend(new_metadata)
        print(f"â• {len(new_embeddings)}ä»¶ã®æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
    else:
        print("â„¹ï¸ æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆé‡è¤‡ãªã—ï¼‰")

    print(f"ğŸ’¾ ãƒ™ã‚¯ãƒˆãƒ«ã‚’ {index_path} ã«ä¿å­˜ä¸­...")
    faiss.write_index(index, index_path)

    print(f"ğŸ’¾ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ {meta_path} ã«ä¿å­˜ä¸­...")
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(existing_metadata, f, ensure_ascii=False, indent=2)

    print("âœ… ä¿å­˜å®Œäº†")

# ========== å®Ÿè¡Œãƒ•ãƒ­ãƒ¼ ==========
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ä½¿ã„æ–¹: python full_pipeline.py <YouTubeã®URL>")
        sys.exit(1)

    url = sys.argv[1]
    audio_filename = "output.mp3"

    download_audio(url, output_path=audio_filename)
    reduce_noise(audio_path=audio_filename, output_path=audio_filename) # å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ã
    segments = transcribe_audio(audio_path=audio_filename, language_code="ja") # æ—¥æœ¬èªã‚’æŒ‡å®š
    embeddings = [embed_text(seg["text"]).squeeze() for seg in tqdm(segments, desc="ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ä¸­")]
    embeddings = np.stack(embeddings).astype("float32")
    save_to_faiss(embeddings, segments)

# HNSWã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¸ã®å¤‰æ›´ï¼ˆç²¾åº¦ã¨é€Ÿåº¦ã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
def create_improved_index(embeddings):
    import faiss
    import numpy as np
    
    dim = embeddings.shape[1]
    
    # HNSWã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ§‹ç¯‰
    # M: ã‚°ãƒ©ãƒ•ã®æ¬¡æ•°ï¼ˆé«˜ã„ã»ã©ç²¾åº¦å‘ä¸Šã ãŒãƒ¡ãƒ¢ãƒªæ¶ˆè²»å¢—åŠ ï¼‰
    # efConstruction: æ§‹ç¯‰æ™‚ã®æ¢ç´¢å¹…ï¼ˆé«˜ã„ã»ã©ç²¾åº¦å‘ä¸Šã ãŒæ§‹ç¯‰æ™‚é–“å¢—åŠ ï¼‰
    index = faiss.IndexHNSWFlat(dim, M=32)
    index.hnsw.efConstruction = 80  # æ§‹ç¯‰æ™‚ã®æ¢ç´¢å¹…
    index.hnsw.efSearch = 128  # æ¤œç´¢æ™‚ã®æ¢ç´¢å¹…ï¼ˆé«˜ã„ã»ã©ç²¾åº¦å‘ä¸Šï¼‰
    
    # åŸ‹ã‚è¾¼ã¿ã‚’è¿½åŠ 
    if len(embeddings) > 0:
        faiss.normalize_L2(embeddings)  # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®ãŸã‚ã®æ­£è¦åŒ–
        index.add(embeddings)
    
    return index

# ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã§ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆç²¾åº¦å‘ä¸Šï¼‰
def rerank_results(query, results, metadata, top_k=5):
    from transformers import AutoModelForSequenceClassification, AutoTokenizer
    import torch
    
    # å¤šè¨€èªã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
    model_name = "amberoad/bert-multilingual-passage-reranking-msmarco"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    
    pairs = []
    ids = []
    
    for idx in results:
        if idx >= 0 and idx < len(metadata):
            text = metadata[idx]["text"]
            pairs.append([query, text])
            ids.append(idx)
    
    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã—ã¦å‡¦ç†
    batch_size = 4
    scores = []
    
    for i in range(0, len(pairs), batch_size):
        batch = pairs[i:i+batch_size]
        inputs = tokenizer(
            batch, 
            padding=True, 
            truncation=True, 
            return_tensors="pt", 
            max_length=512
        )
        
        with torch.no_grad():
            outputs = model(**inputs)
            batch_scores = outputs.logits.squeeze(-1).tolist()
            scores.extend(batch_scores)
    
    # ã‚¹ã‚³ã‚¢ã¨å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ã‚½ãƒ¼ãƒˆ
    scored_results = sorted(zip(ids, scores), key=lambda x: x[1], reverse=True)
    return [idx for idx, _ in scored_results[:top_k]]