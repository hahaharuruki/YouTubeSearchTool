import subprocess
import whisper
import faiss
import numpy as np
import json
import os
import sys
from tqdm import tqdm
import noisereduce as nr
import librosa
import soundfile as sf
import torch
import whisperx

def extract_video_id(url):
    import re
    m = re.search(r"v=([a-zA-Z0-9_\-]+)", url)
    return m.group(1) if m else url

def get_video_title(youtube_url):
    import subprocess
    try:
        result = subprocess.run(
            ["yt-dlp", "--get-title", youtube_url],
            capture_output=True, text=True, check=True
        )
        return result.stdout.strip()
    except Exception:
        return ""

# ========== 1. å‹•ç”»ã‹ã‚‰éŸ³å£°ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ==========
def download_audio(youtube_url, output_path="output.mp3"):
    if os.path.exists(output_path):
        print(f"ğŸ—‘ï¸ æ—¢å­˜ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ« {output_path} ã‚’å‰Šé™¤ã—ã¾ã™...")
        os.remove(output_path)

    print("ğŸ§ éŸ³å£°ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
    subprocess.run([
        "yt-dlp",
        "--no-cache-dir",
        "-x", "--audio-format", "mp3",
        "-o", output_path,
        youtube_url
    ])
    print(f"âœ… éŸ³å£°ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")

def denoise_audio(input_path="output.mp3", output_path="output_denoised.wav"):
    print("ğŸ§¹ ãƒã‚¤ã‚ºé™¤å»ä¸­...")
    y, sr = librosa.load(input_path, sr=16000)
    reduced_noise = nr.reduce_noise(y=y, sr=sr)
    sf.write(output_path, reduced_noise, sr)
    print(f"âœ… ãƒã‚¤ã‚ºé™¤å»æ¸ˆã¿éŸ³å£°ã‚’ä¿å­˜: {output_path}")

# ========== 2. WhisperXã§æ–‡å­—èµ·ã“ã—ï¼‹è©±è€…åˆ†é›¢ ==========
def transcribe_audio_with_speaker(audio_path):
    hf_token = os.environ.get("HF_TOKEN")
    if not hf_token:
        # ãƒˆãƒ¼ã‚¯ãƒ³ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å…¥åŠ›ã‚’ä¿ƒã™ã‹ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã—ã¦çµ‚äº†ã™ã‚‹
        # ã“ã“ã§ã¯ä¾‹ã¨ã—ã¦Noneã‚’æ¸¡ã™ãŒã€å®Ÿéš›ã«ã¯é©åˆ‡ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒå¿…è¦
        print("è­¦å‘Š: Hugging Faceã®ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ãŒç’°å¢ƒå¤‰æ•° HF_TOKEN ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        print("è©±è€…åˆ†é›¢æ©Ÿèƒ½ãŒæ­£ã—ãå‹•ä½œã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        # raise ValueError("Hugging Faceã®ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³(HF_TOKEN)ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚") # ã‚ˆã‚Šå³æ ¼ãªå ´åˆ
    print("ğŸ“ WhisperXã§æ–‡å­—èµ·ã“ã—ï¼‹è©±è€…åˆ†é›¢ä¸­...")
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # 1. æ–‡å­—èµ·ã“ã—å®Ÿè¡Œ
    model = whisperx.load_model("large-v3", device, compute_type="float32")
    transcription_result = model.transcribe(audio_path) # æ–‡å­—èµ·ã“ã—çµæœ (è¾æ›¸å‹)
    print(f"âœ… æ–‡å­—èµ·ã“ã—å®Œäº†: {len(transcription_result['segments'])}ä»¶ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å–å¾—")
    language_code = transcription_result["language"]
    print(f"ğŸ—£ï¸ æ¤œå‡ºã•ã‚ŒãŸè¨€èª: {language_code}")

    # 2. ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨å®Ÿè¡Œ
    print("ğŸ”„ æ–‡å­—èµ·ã“ã—çµæœã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆä¸­...")
    try:
        align_model, metadata = whisperx.load_align_model(language_code=language_code, device=device)
        aligned_result = whisperx.align(transcription_result["segments"], align_model, metadata, audio_path, device)
        print(f"âœ… ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆå®Œäº†: {len(aligned_result['segments'])}ä»¶ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ")
        # ãƒ¡ãƒ¢ãƒªè§£æ”¾
        del align_model
        import gc
        gc.collect()
    except Exception as e:
        print(f"âš ï¸ ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print("ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãªã—ã§å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™ã€‚")
        aligned_result = transcription_result # ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆå¤±æ•—æ™‚ã¯å…ƒã®æ–‡å­—èµ·ã“ã—çµæœã‚’ä½¿ç”¨

    # 3. è©±è€…åˆ†é›¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æº–å‚™ã¨å®Ÿè¡Œ
    diarize_model_instance = whisperx.diarize.DiarizationPipeline(device=device, use_auth_token=hf_token)
    print("ğŸ—£ï¸ è©±è€…åˆ†é›¢ã‚’å®Ÿè¡Œä¸­...")
    # diarize_model_instance ã«ã¯éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ã¿ã‚’æ¸¡ã™
    diarization_annotation = diarize_model_instance(audio_path)
    # 4. ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã•ã‚ŒãŸæ–‡å­—èµ·ã“ã—çµæœã«è©±è€…æƒ…å ±ã‚’å‰²ã‚Šå½“ã¦
    print("ğŸ”— æ–‡å­—èµ·ã“ã—çµæœã«è©±è€…æƒ…å ±ã‚’å‰²ã‚Šå½“ã¦ä¸­...")
    final_result_with_speakers = whisperx.assign_word_speakers(diarization_annotation, aligned_result)
    segments = final_result_with_speakers["segments"]
    print(f"âœ… è©±è€…ãƒ©ãƒ™ãƒ«ä»˜ãã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {len(segments)}")
    return segments

# ========== 3. åŸ‹ã‚è¾¼ã¿ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰ ==========
def embed_text(text):
    import torch
    from transformers import AutoTokenizer, AutoModel
    
    # ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›ï¼ˆé‡ã¿ä»˜ã‘ï¼‰
    models = [
        ("BAAI/bge-large-en-v1.5", 0.6),  # åŸºæœ¬ãƒ¢ãƒ‡ãƒ«
        ("intfloat/multilingual-e5-large", 0.4)  # æ—¥æœ¬èªå¯¾å¿œè£œå®Œ
    ]
    
    final_embedding = None
    device = torch.device("cpu")  # MPSã¯ä¸å®‰å®šãªã®ã§CPUæ¨å¥¨
    
    for model_name, weight in models:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name).to(device)
        
        inputs = tokenizer(
            text, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512, 
            padding=True
        ).to(device)
        
        with torch.no_grad():
            outputs = model(**inputs)
            embedding = outputs.last_hidden_state[:, 0].cpu().numpy()
        
        if final_embedding is None:
            final_embedding = embedding * weight
        else:
            # æ¬¡å…ƒãŒç•°ãªã‚‹å ´åˆã®èª¿æ•´ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
            if embedding.shape != final_embedding.shape:
                from sklearn.preprocessing import normalize
                # ãƒªã‚µã‚¤ã‚ºã—ã¦æ­£è¦åŒ–
                embedding = normalize(embedding).reshape(final_embedding.shape)
            
            final_embedding += embedding * weight
        
        # ãƒ¡ãƒ¢ãƒªè§£æ”¾
        del model, tokenizer
        import gc
        gc.collect()
    
    return final_embedding

# ========== 4. FAISS ã«æ ¼ç´ ==========
def save_to_faiss(embeddings, segments, index_path="faiss.index", meta_path="metadata.json"):
    # Load existing index if exists
    if os.path.exists(index_path):
        print(f"ğŸ“‚ æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {index_path} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        index = faiss.read_index(index_path)
    else:
        dim = embeddings.shape[1]
        index = faiss.IndexFlatL2(dim)

    # Load existing metadata if exists
    if os.path.exists(meta_path):
        print(f"ğŸ“‚ æ—¢å­˜ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ {meta_path} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        with open(meta_path, "r", encoding="utf-8") as f:
            existing_metadata = json.load(f)
    else:
        existing_metadata = []

    # Prepare a set of existing entries for quick lookup to avoid duplicates
    existing_set = set((m["start"], m["end"], m["text"]) for m in existing_metadata)

    # Prepare a dict to map video_id to set of titles for existing metadata
    video_titles_map = {}
    for meta in existing_metadata:
        vid = meta.get("video_id")
        titles = meta.get("video_titles", [])
        if vid:
            if vid not in video_titles_map:
                video_titles_map[vid] = set()
            video_titles_map[vid].update(titles)

    # Filter new segments and embeddings to exclude duplicates
    new_metadata = []
    new_embeddings = []
    for seg, emb in zip(segments, embeddings):
        key = (seg["start"], seg["end"], seg["text"])
        if key not in existing_set:
            vid = seg.get("video_id")
            vtitle = seg.get("video_title", "")
            # Prepare video_titles list for this segment
            if vid:
                old_titles = video_titles_map.get(vid, set())
                updated_titles = set(old_titles)
                if vtitle:
                    updated_titles.add(vtitle)
                seg["video_titles"] = list(updated_titles)
                video_titles_map[vid] = updated_titles
            else:
                seg["video_titles"] = [vtitle] if vtitle else []
            new_metadata.append(seg)
            new_embeddings.append(emb)
            existing_set.add(key)
        else:
            # Even if duplicate, update video_titles in existing metadata if needed
            for meta in existing_metadata:
                if (meta["start"], meta["end"], meta["text"]) == key:
                    vid = seg.get("video_id")
                    vtitle = seg.get("video_title", "")
                    if vid:
                        old_titles = set(meta.get("video_titles", []))
                        if vtitle and vtitle not in old_titles:
                            old_titles.add(vtitle)
                            meta["video_titles"] = list(old_titles)
                    break

    if len(new_embeddings) > 0:
        new_embeddings_np = np.array(new_embeddings).astype("float32")
        index.add(new_embeddings_np)
        existing_metadata.extend(new_metadata)
        print(f"â• {len(new_embeddings)}ä»¶ã®æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
    else:
        print("â„¹ï¸ æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆé‡è¤‡ãªã—ï¼‰")

    print(f"ğŸ’¾ ãƒ™ã‚¯ãƒˆãƒ«ã‚’ {index_path} ã«ä¿å­˜ä¸­...")
    faiss.write_index(index, index_path)

    print(f"ğŸ’¾ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ {meta_path} ã«ä¿å­˜ä¸­...")
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(existing_metadata, f, ensure_ascii=False, indent=2)

    print("âœ… ä¿å­˜å®Œäº†")

# ========== å®Ÿè¡Œãƒ•ãƒ­ãƒ¼ ==========
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ä½¿ã„æ–¹: python full_pipeline.py <YouTubeã®URL>")
        sys.exit(1)

    url = sys.argv[1]
    video_id = extract_video_id(url)
    video_title = get_video_title(url)
    download_audio(url)
    denoise_audio("output.mp3", "output_denoised.wav")
    segments = transcribe_audio_with_speaker("output_denoised.wav")
    for seg in segments:
        seg["video_id"] = video_id
        seg["video_url"] = url
        seg["video_title"] = video_title
    embeddings = [embed_text(seg["text"]).squeeze() for seg in segments]
    embeddings = np.stack(embeddings).astype("float32")
    save_to_faiss(embeddings, segments)

# Silero VADã¸å¤‰æ›´ï¼ˆç²¾åº¦å‘ä¸Šï¼‰
def get_vad_segments(audio_path):
    import torch
    model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',
                                 model='silero_vad',
                                 force_reload=True)
    (get_speech_timestamps, _, _, _, _) = utils
    
    # ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã®èª­ã¿è¾¼ã¿ï¼ˆ16kHzã€ãƒ¢ãƒãƒ©ãƒ«åŒ–ï¼‰
    import librosa
    audio, sr = librosa.load(audio_path, sr=16000, mono=True)
    
    # éŸ³å£°åŒºé–“ã®æ¤œå‡º
    speech_timestamps = get_speech_timestamps(
        torch.tensor(audio),
        model,
        threshold=0.5,  # æ„Ÿåº¦èª¿æ•´ï¼ˆ0.3-0.7ã®é–“ã§èª¿æ•´å¯èƒ½ï¼‰
        sampling_rate=16000
    )
    
    return speech_timestamps

# HNSWã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¸ã®å¤‰æ›´ï¼ˆç²¾åº¦ã¨é€Ÿåº¦ã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
def create_improved_index(embeddings):
    import faiss
    import numpy as np
    
    dim = embeddings.shape[1]
    
    # HNSWã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ§‹ç¯‰
    # M: ã‚°ãƒ©ãƒ•ã®æ¬¡æ•°ï¼ˆé«˜ã„ã»ã©ç²¾åº¦å‘ä¸Šã ãŒãƒ¡ãƒ¢ãƒªæ¶ˆè²»å¢—åŠ ï¼‰
    # efConstruction: æ§‹ç¯‰æ™‚ã®æ¢ç´¢å¹…ï¼ˆé«˜ã„ã»ã©ç²¾åº¦å‘ä¸Šã ãŒæ§‹ç¯‰æ™‚é–“å¢—åŠ ï¼‰
    index = faiss.IndexHNSWFlat(dim, M=32)
    index.hnsw.efConstruction = 80  # æ§‹ç¯‰æ™‚ã®æ¢ç´¢å¹…
    index.hnsw.efSearch = 128  # æ¤œç´¢æ™‚ã®æ¢ç´¢å¹…ï¼ˆé«˜ã„ã»ã©ç²¾åº¦å‘ä¸Šï¼‰
    
    # åŸ‹ã‚è¾¼ã¿ã‚’è¿½åŠ 
    if len(embeddings) > 0:
        faiss.normalize_L2(embeddings)  # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®ãŸã‚ã®æ­£è¦åŒ–
        index.add(embeddings)
    
    return index

# ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã§ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆç²¾åº¦å‘ä¸Šï¼‰
def rerank_results(query, results, metadata, top_k=5):
    from transformers import AutoModelForSequenceClassification, AutoTokenizer
    import torch
    
    # å¤šè¨€èªã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
    model_name = "amberoad/bert-multilingual-passage-reranking-msmarco"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    
    pairs = []
    ids = []
    
    for idx in results:
        if idx >= 0 and idx < len(metadata):
            text = metadata[idx]["text"]
            pairs.append([query, text])
            ids.append(idx)
    
    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã—ã¦å‡¦ç†
    batch_size = 4
    scores = []
    
    for i in range(0, len(pairs), batch_size):
        batch = pairs[i:i+batch_size]
        inputs = tokenizer(
            batch, 
            padding=True, 
            truncation=True, 
            return_tensors="pt", 
            max_length=512
        )
        
        with torch.no_grad():
            outputs = model(**inputs)
            batch_scores = outputs.logits.squeeze(-1).tolist()
            scores.extend(batch_scores)
    
    # ã‚¹ã‚³ã‚¢ã¨å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ã‚½ãƒ¼ãƒˆ
    scored_results = sorted(zip(ids, scores), key=lambda x: x[1], reverse=True)
    return [idx for idx, _ in scored_results[:top_k]]